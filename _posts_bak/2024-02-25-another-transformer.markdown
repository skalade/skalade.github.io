---
layout: post
title: "Yet another transformer post"
subtitle: "walking through the transformer architecture so I finally understand it"
date: 2024-02-25 13:45:13 -0400
background: '/assets/img/posts/01_transformer_by_dalle.jpg'
---

# Intro

Since the recent revolution in ML that has been the transformer architecture there have been thousands of blogs/articles/videos explaining the architecture. And this is another one. I found myself frustrated trying to grok attention and how oh how does an architecture based around fully connected layers is applicable to variable length sequences.

I figure the best way to learn is to explain, so I'm just gonna run through it from the perspective of someone who's worked with Seq2Seq models before but has been out of the game for a while... Best way to learn is to teach and if this ends up being useful to anyone else on the internet then that's a bonus.

## Sequence to Sequence modelling

![](/assets/img/01/encoder_decoder.png)

### The RNN


### Encoder-Decoder structure


### Limitations of RNNs

## The transformer

### Encoder

### Decoder